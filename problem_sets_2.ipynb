{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import transformers\n",
    "import torch\n",
    "import dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:46<00:00, 11.74s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    token=os.getenv('HF_TOKEN')\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule 110 Cellular Automaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cellular_problem_set(size, steps, num_problems, boundary='wrap'):\n",
    "    if steps < 3:\n",
    "        raise ValueError(\"Need 3 steps to have a solution and 2 intermediates\")\n",
    "    def int_to_binary_list(n, min_length=8):\n",
    "        binary = bin(n)[2:]  # Convert to binary string and remove '0b' prefix\n",
    "        binary_list = [int(b) for b in binary.zfill(min_length)]  # Pad with zeros if necessary\n",
    "        return binary_list\n",
    "    \n",
    "    dict_110 = {\n",
    "        (0, 0, 0): 0,\n",
    "        (0, 0, 1): 1,\n",
    "        (0, 1, 0): 1,\n",
    "        (0, 1, 1): 1,\n",
    "        (1, 0, 0): 0,\n",
    "        (1, 0, 1): 1,\n",
    "        (1, 1, 0): 1,\n",
    "        (1, 1, 1): 0\n",
    "    }\n",
    "\n",
    "    def rule_110(prev):\n",
    "        next_state = []\n",
    "        for i in range(len(prev)):\n",
    "            left = prev[(i-1) % len(prev)] if boundary == 'wrap' or (i > 0 and i < len(prev)-1) else boundary\n",
    "            center = prev[i]\n",
    "            right = prev[(i+1) % len(prev)] if boundary == 'wrap' or (i > 0 and i < len(prev)-1) else boundary\n",
    "            pattern = (left, center, right)\n",
    "            next_state.append(dict_110[pattern])\n",
    "        return next_state\n",
    "    \n",
    "    def make_rule_110_problem(initial_state, steps):\n",
    "        current_state = initial_state\n",
    "        states = [current_state]\n",
    "        for _ in range(steps):\n",
    "            current_state = rule_110(current_state)\n",
    "            states.append(current_state)\n",
    "        return (''.join(str(x) for x in initial_state),\n",
    "            ''.join(str(x) for x in states[-1]),\n",
    "            ''.join(str(x) for x in states[1]),\n",
    "            ''.join(str(x) for x in states[-2]))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [make_rule_110_problem(int_to_binary_list(((i+1)*33581)%(2**(size))), steps) for i in range(num_problems)]\n",
    "        , columns=['problem', 'correct_solution', 'intermediate_1', 'intermediate_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>correct_solution</th>\n",
       "      <th>intermediate_1</th>\n",
       "      <th>intermediate_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100101101</td>\n",
       "      <td>1001000011</td>\n",
       "      <td>1101111111</td>\n",
       "      <td>1111000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001011010</td>\n",
       "      <td>1010000110</td>\n",
       "      <td>1011111110</td>\n",
       "      <td>1110000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110000111</td>\n",
       "      <td>110110001</td>\n",
       "      <td>110001101</td>\n",
       "      <td>110011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10110100</td>\n",
       "      <td>10001100</td>\n",
       "      <td>11111100</td>\n",
       "      <td>10000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1111100001</td>\n",
       "      <td>1011101101</td>\n",
       "      <td>1000100011</td>\n",
       "      <td>1001100111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1100010100</td>\n",
       "      <td>1111101100</td>\n",
       "      <td>1100111100</td>\n",
       "      <td>1101100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1001000001</td>\n",
       "      <td>1001001101</td>\n",
       "      <td>1011000011</td>\n",
       "      <td>1111000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>101101110</td>\n",
       "      <td>100011010</td>\n",
       "      <td>111111010</td>\n",
       "      <td>100001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>10011011</td>\n",
       "      <td>10100011</td>\n",
       "      <td>10111111</td>\n",
       "      <td>11100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1111001000</td>\n",
       "      <td>1110001000</td>\n",
       "      <td>1001011000</td>\n",
       "      <td>1011111000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        problem correct_solution intermediate_1 intermediate_2\n",
       "0    1100101101       1001000011     1101111111     1111000001\n",
       "1    1001011010       1010000110     1011111110     1110000010\n",
       "2     110000111        110110001      110001101      110011111\n",
       "3      10110100         10001100       11111100       10000100\n",
       "4    1111100001       1011101101     1000100011     1001100111\n",
       "..          ...              ...            ...            ...\n",
       "995  1100010100       1111101100     1100111100     1101100100\n",
       "996  1001000001       1001001101     1011000011     1111000111\n",
       "997   101101110        100011010      111111010      100001110\n",
       "998    10011011         10100011       10111111       11100001\n",
       "999  1111001000       1110001000     1001011000     1011111000\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cellular_problem_set(10, 3, 1000, boundary=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysat.formula import CNF\n",
    "from pysat.solvers import Glucose3\n",
    "\n",
    "def solve_nsat(clauses):\n",
    "    # Create a CNF formula\n",
    "    cnf = CNF()\n",
    "    for clause in clauses:\n",
    "        cnf.append(clause)\n",
    "\n",
    "    # Create a SAT solver\n",
    "    with Glucose3(bootstrap_with=cnf) as solver:\n",
    "        # Check if the formula is satisfiable\n",
    "        if solver.solve():\n",
    "            return solver.get_model()\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nsat_problem_set(vars_per_clause, num_clauses, num_problems):\n",
    "    def make_nsat_problem(vars_per_clause, num_clauses):\n",
    "        text_variables = [string.ascii_lowercase[i] for i in range(vars_per_clause)]\n",
    "        text_problem = []\n",
    "        pysat_problem = []\n",
    "        for _ in range(num_clauses):\n",
    "            clause = random.sample(range(vars_per_clause), 3)\n",
    "            signs = [random.choice([-1, 1]) for _ in range(3)]\n",
    "            pysat_clause = [signs[i]*(var+1) for i, var in enumerate(clause)]\n",
    "            pysat_problem.append(pysat_clause)\n",
    "            pysat_solution = solve_nsat(pysat_problem)\n",
    "            if pysat_solution is None:\n",
    "                text_solution = None\n",
    "            else:\n",
    "                text_solution_letters = [f\"{'¬' if var <0 else ''}{text_variables[abs(var)-1]}\" for i, var in enumerate(pysat_solution)]\n",
    "                text_solution = f\"{' ^ '.join(text_solution_letters)}\"\n",
    "\n",
    "            text_clause = [f\"{'¬' if signs[i] == -1 else ''}{text_variables[var]}\" for i, var in enumerate(clause)]\n",
    "            text_problem.append(f\"({' v '.join(text_clause)})\")\n",
    "        return ' ^ '.join(text_problem), text_solution, None, None\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [make_nsat_problem(vars_per_clause, num_clauses) for _ in range(num_problems)],\n",
    "        columns=['problem', 'correct_solution', 'intermediate_1', 'intermediate_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>correct_solution</th>\n",
       "      <th>intermediate_1</th>\n",
       "      <th>intermediate_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(c v b v ¬a) ^ (a v ¬c v ¬b) ^ (b v ¬a v ¬c) ^...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(¬c v ¬a v ¬b) ^ (a v ¬b v ¬c) ^ (¬b v ¬a v c)...</td>\n",
       "      <td>¬a ^ ¬b ^ ¬c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a v c v b) ^ (a v c v ¬b) ^ (b v ¬a v c) ^ (b...</td>\n",
       "      <td>a ^ ¬b ^ c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(¬c v b v a) ^ (b v ¬a v ¬c) ^ (a v c v b) ^ (...</td>\n",
       "      <td>a ^ ¬b ^ ¬c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(¬b v c v a) ^ (c v ¬a v b) ^ (¬c v ¬b v a) ^ ...</td>\n",
       "      <td>a ^ b ^ ¬c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>(¬b v ¬c v ¬a) ^ (c v ¬a v ¬b) ^ (¬a v ¬b v ¬c...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>(¬c v ¬a v ¬b) ^ (¬c v a v b) ^ (¬b v c v ¬a) ...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>(¬a v c v ¬b) ^ (b v ¬c v ¬a) ^ (¬c v b v ¬a) ...</td>\n",
       "      <td>¬a ^ b ^ c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>(¬b v a v c) ^ (b v ¬c v a) ^ (¬b v ¬c v a) ^ ...</td>\n",
       "      <td>a ^ b ^ ¬c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>(¬a v c v ¬b) ^ (b v a v ¬c) ^ (c v b v ¬a) ^ ...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               problem correct_solution  \\\n",
       "0    (c v b v ¬a) ^ (a v ¬c v ¬b) ^ (b v ¬a v ¬c) ^...                    \n",
       "1    (¬c v ¬a v ¬b) ^ (a v ¬b v ¬c) ^ (¬b v ¬a v c)...     ¬a ^ ¬b ^ ¬c   \n",
       "2    (a v c v b) ^ (a v c v ¬b) ^ (b v ¬a v c) ^ (b...       a ^ ¬b ^ c   \n",
       "3    (¬c v b v a) ^ (b v ¬a v ¬c) ^ (a v c v b) ^ (...      a ^ ¬b ^ ¬c   \n",
       "4    (¬b v c v a) ^ (c v ¬a v b) ^ (¬c v ¬b v a) ^ ...       a ^ b ^ ¬c   \n",
       "..                                                 ...              ...   \n",
       "995  (¬b v ¬c v ¬a) ^ (c v ¬a v ¬b) ^ (¬a v ¬b v ¬c...                    \n",
       "996  (¬c v ¬a v ¬b) ^ (¬c v a v b) ^ (¬b v c v ¬a) ...                    \n",
       "997  (¬a v c v ¬b) ^ (b v ¬c v ¬a) ^ (¬c v b v ¬a) ...       ¬a ^ b ^ c   \n",
       "998  (¬b v a v c) ^ (b v ¬c v a) ^ (¬b v ¬c v a) ^ ...       a ^ b ^ ¬c   \n",
       "999  (¬a v c v ¬b) ^ (b v a v ¬c) ^ (c v b v ¬a) ^ ...                    \n",
       "\n",
       "    intermediate_1 intermediate_2  \n",
       "0             None           None  \n",
       "1             None           None  \n",
       "2             None           None  \n",
       "3             None           None  \n",
       "4             None           None  \n",
       "..             ...            ...  \n",
       "995           None           None  \n",
       "996           None           None  \n",
       "997           None           None  \n",
       "998           None           None  \n",
       "999           None           None  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_nsat_problem_set(3, 20, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dot_product_problem_set(vec_len, vec_mag, num_problems):\n",
    "    if vec_len < 2:\n",
    "        raise ValueError(\"Need vectors of length 2 or greater to have two intermediates\")\n",
    "    def make_dot_product_problem(vec_len):\n",
    "        a = np.random.randint(-vec_mag, vec_mag, vec_len)\n",
    "        b = np.random.randint(-vec_mag, vec_mag, vec_len)\n",
    "        return (f\"[{', '.join([str(x) for x in a])}] ⋅ [{', '.join([str(x) for x in b])}]\"\n",
    "        , np.dot(a, b)\n",
    "        , a[0]*b[0],\n",
    "        a[-1]*b[-1])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [make_dot_product_problem(vec_len) for _ in range(num_problems)],\n",
    "        columns=['problem', 'correct_solution', 'intermediate_1', 'intermediate_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>correct_solution</th>\n",
       "      <th>intermediate_1</th>\n",
       "      <th>intermediate_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-5, 5, 7] ⋅ [9, 1, 8]</td>\n",
       "      <td>16</td>\n",
       "      <td>-45</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 3, 4] ⋅ [3, 6, 8]</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, -1, 0] ⋅ [-8, -8, 5]</td>\n",
       "      <td>-24</td>\n",
       "      <td>-32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, -2, 0] ⋅ [-8, 6, -7]</td>\n",
       "      <td>-84</td>\n",
       "      <td>-72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-3, 8, -2] ⋅ [-9, -1, 1]</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[-8, 1, 9] ⋅ [3, 9, -2]</td>\n",
       "      <td>-33</td>\n",
       "      <td>-24</td>\n",
       "      <td>-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[0, 5, 3] ⋅ [-3, 9, 8]</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[-8, -4, -1] ⋅ [9, -5, 8]</td>\n",
       "      <td>-60</td>\n",
       "      <td>-72</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[4, 0, -8] ⋅ [-1, 4, 6]</td>\n",
       "      <td>-52</td>\n",
       "      <td>-4</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[2, 6, 0] ⋅ [-3, -4, -4]</td>\n",
       "      <td>-30</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       problem  correct_solution  intermediate_1  \\\n",
       "0       [-5, 5, 7] ⋅ [9, 1, 8]                16             -45   \n",
       "1        [5, 3, 4] ⋅ [3, 6, 8]                65              15   \n",
       "2     [4, -1, 0] ⋅ [-8, -8, 5]               -24             -32   \n",
       "3     [9, -2, 0] ⋅ [-8, 6, -7]               -84             -72   \n",
       "4    [-3, 8, -2] ⋅ [-9, -1, 1]                17              27   \n",
       "..                         ...               ...             ...   \n",
       "995    [-8, 1, 9] ⋅ [3, 9, -2]               -33             -24   \n",
       "996     [0, 5, 3] ⋅ [-3, 9, 8]                69               0   \n",
       "997  [-8, -4, -1] ⋅ [9, -5, 8]               -60             -72   \n",
       "998    [4, 0, -8] ⋅ [-1, 4, 6]               -52              -4   \n",
       "999   [2, 6, 0] ⋅ [-3, -4, -4]               -30              -6   \n",
       "\n",
       "     intermediate_2  \n",
       "0                56  \n",
       "1                32  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                -2  \n",
       "..              ...  \n",
       "995             -18  \n",
       "996              24  \n",
       "997              -8  \n",
       "998             -48  \n",
       "999               0  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_problems = make_dot_product_problem_set(3, 10, 1000)\n",
    "dot_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem_cot(problem, sys_prompt, cot_prompt):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_prompt + ' ' + cot_prompt},\n",
    "    {\"role\": \"user\", \"content\": problem},\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return outputs[0]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/generation/utils.py:1513: UserWarning: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  if eos_token_id is not None and torch.isin(elements=eos_token_id, test_elements=pad_token_id).any():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/james/Documents/My Documents/Projects/SPAR SU24/stego-snowball/problem_sets_2.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m dot_problems\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(solve_problem_cot(row[\u001b[39m'\u001b[39;49m\u001b[39mproblem\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the dot product of these two vectors?\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mShow your work.\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32m/Users/james/Documents/My Documents/Projects/SPAR SU24/stego-snowball/problem_sets_2.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve_problem_cot\u001b[39m(problem, sys_prompt, cot_prompt):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     messages \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: sys_prompt \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m cot_prompt},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: problem},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     outputs \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     messages,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mterminators,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/SPAR%20SU24/stego-snowball/problem_sets_2.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:257\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    253\u001b[0m     text_inputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, KeyDataset) \u001b[39mif\u001b[39;00m is_torch_available() \u001b[39melse\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)\n\u001b[1;32m    254\u001b[0m ) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text_inputs[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mdict\u001b[39m)):\n\u001b[1;32m    255\u001b[0m     \u001b[39m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text_inputs[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(Chat(text_inputs), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         chats \u001b[39m=\u001b[39m [Chat(chat) \u001b[39mfor\u001b[39;00m chat \u001b[39min\u001b[39;00m text_inputs]  \u001b[39m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/pipelines/base.py:1254\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1247\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1248\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         )\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/pipelines/base.py:1261\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1260\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1261\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1262\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1263\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/pipelines/base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1160\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1161\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1162\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1163\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:349\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    348\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    350\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1915\u001b[0m         input_ids,\n\u001b[1;32m   1916\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1917\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1918\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1919\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1920\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1921\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1922\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config, device\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2652\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2653\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2654\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2655\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2656\u001b[0m )\n\u001b[1;32m   2658\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1175\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1176\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1177\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1178\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1179\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1180\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1181\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1182\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1183\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1184\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    967\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    968\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    969\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         cache_position,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    979\u001b[0m         hidden_states,\n\u001b[1;32m    980\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    981\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    982\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    983\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    984\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    985\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    988\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:718\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    717\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    719\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    720\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    721\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    722\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    723\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    724\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    725\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    729\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:614\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(\n\u001b[1;32m    603\u001b[0m         hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    604\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         cache_position\u001b[39m=\u001b[39mcache_position,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    612\u001b[0m bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[0;32m--> 614\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states)\n\u001b[1;32m    615\u001b[0m key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    616\u001b[0m value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/hooks.py:161\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(module, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     args, kwargs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_hf_hook\u001b[39m.\u001b[39;49mpre_forward(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mno_grad:\n\u001b[1;32m    163\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/hooks.py:328\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m name, _ \u001b[39min\u001b[39;00m named_module_tensors(\n\u001b[1;32m    322\u001b[0m     module,\n\u001b[1;32m    323\u001b[0m     include_buffers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload_buffers,\n\u001b[1;32m    324\u001b[0m     recurse\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_submodules,\n\u001b[1;32m    325\u001b[0m     remove_non_persistent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m ):\n\u001b[1;32m    327\u001b[0m     fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights_map[name]\n\u001b[1;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name \u001b[39mand\u001b[39;00m name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_map\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    330\u001b[0m         \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mint8:\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/utils/offload.py:118\u001b[0m, in \u001b[0;36mPrefixedDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefix\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mkey\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[0;32m~/miniconda3/envs/censored-cognition/lib/python3.11/site-packages/accelerate/utils/offload.py:171\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(weight_info[\u001b[39m\"\u001b[39m\u001b[39msafetensors_file\u001b[39m\u001b[39m\"\u001b[39m], framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 171\u001b[0m         tensor \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mget_tensor(weight_info\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mweight_name\u001b[39m\u001b[39m\"\u001b[39m, key))\n\u001b[1;32m    172\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[39m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(weight_info[\u001b[39m\"\u001b[39m\u001b[39msafetensors_file\u001b[39m\u001b[39m\"\u001b[39m], framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, row in dot_problems.iterrows():\n",
    "    print(solve_problem_cot(row['problem'], \"What is the dot product of these two vectors?\", \"Show your work.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censored-cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
